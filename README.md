# ğŸŸ Sports Data Warehouse â€” Football Analytics

Proyecto de **Data Engineering / Analytics Engineering** enfocado en construir un **Data Warehouse incremental** a partir de datos de fÃºtbol obtenidos vÃ­a API, utilizando **DuckDB**, SQL y Python.

El objetivo principal es demostrar:

* modelado dimensional correcto (star schema)
* cargas incrementales reales (no full reloads)
* control de snapshots histÃ³ricos
* detecciÃ³n y correcciÃ³n de duplicados
* diseÃ±o defendible en entrevistas tÃ©cnicas

---

## ğŸ§± Arquitectura general

```
raw/            â†’ JSON crudo desde API
staging/        â†’ normalizaciÃ³n y limpieza
warehouse/      â†’ modelo estrella final
```

**TecnologÃ­as**:

* DuckDB
* SQL (MERGE, window functions)
* Python

---

## â­ Modelo dimensional (Star Schema)

### Dimensiones

* `dim_player`
* `dim_team`
* `dim_league`

### Hechos

* `fact_player_stats`

**Grain del fact**:

> 1 fila por **player + team + league + season**

---

## ğŸ”„ Pipeline ETL

### 1ï¸âƒ£ Ingesta RAW

Datos obtenidos desde API Football y almacenados como JSON:

```
raw/api_football/players/league=39/season=2023/snapshot=YYYY-MM-DD_page=X.json
```

Cada snapshot representa el estado completo de la API en una fecha determinada.

---

### 2ï¸âƒ£ Staging

NormalizaciÃ³n y limpieza de JSON:

* flatten de estructuras anidadas
* casteos de tipos
* extracciÃ³n de `snapshot_date` desde filename

Tablas clave:

* `stg_players`
* `stg_players_clean`
* `stg_players_incremental`
* `stg_players_incremental_clean`

---

### 3ï¸âƒ£ Incremental load (core del proyecto)

Se implementan cargas incrementales reales usando:

* snapshots
* tabla de control
* `MERGE INTO`

#### Tabla de control

```sql
etl_control(
  source_name,
  last_snapshot
)
```

Permite procesar **solo nuevos snapshots**.

---

### 4ï¸âƒ£ Merge en dimensiones

Ejemplo: `dim_player`

* **UPDATE** si existe el player y el snapshot es mÃ¡s nuevo
* **INSERT** si el player no existe

Esto permite:

* mantener dimensiones actualizadas
* evitar duplicados

---

### 5ï¸âƒ£ Merge en fact

`fact_player_stats` se carga incrementalmente usando:

* keys surrogate
* comparaciÃ³n de snapshot

Se corrigieron duplicados histÃ³ricos mediante:

* `ROW_NUMBER()`
* limpieza one-time

---

## ğŸ§ª Data Quality Checks

Algunos checks implementados:

* igualdad entre total rows y distinct grain
* detecciÃ³n de NULLs crÃ­ticos
* control de duplicados post-merge

Ejemplo:

```sql
COUNT(*) = COUNT(DISTINCT player_key, team_key, league_key, season)
```

---

## ğŸ§  Decisiones de diseÃ±o

* DuckDB elegido por simplicidad y potencia analÃ­tica
* snapshots completos para garantizar consistencia
* MERGE para simular pipelines productivos
* evitar herramientas externas (Airflow) para foco conceptual

---

## ğŸ“ˆ Estado actual

* âœ” Dimensiones incrementales
* âœ” Fact incremental
* âœ” Control de snapshots
* âœ” Warehouse consistente

---

## ğŸ”® PrÃ³ximos pasos

* Queries BI (top scorers, evoluciÃ³n temporal)
* DockerizaciÃ³n
* OrquestaciÃ³n (Airflow / Dagster)
* Tests automÃ¡ticos

---

## ğŸ¯ Objetivo del proyecto

Este proyecto estÃ¡ pensado para:

* entrevistas tÃ©cnicas de Data Engineer / Analytics Engineer
* demostrar dominio real de ETL incremental
* servir como base para anÃ¡lisis BI

---

ğŸ‘¤ Autor: *Thiago*



